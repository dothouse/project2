{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPuzNJrHdprAqpua1dvb/np"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["https://real-myeong.tistory.com/46"],"metadata":{"id":"w_m1sO2heJsX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","base_path = '/content/drive/MyDrive/new_project/project2'\n","traindata_path = '/content/drive/MyDrive/new_project/project2/traindata/문서요약 텍스트/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8dF9E2weNi0","executionInfo":{"status":"ok","timestamp":1714459347719,"user_tz":-540,"elapsed":2374,"user":{"displayName":"JinSeok Song","userId":"14796603013551188771"}},"outputId":"d7976448-ba72-4ffa-d667-1c34eaedfd4a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","from bs4 import BeautifulSoup\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import urllib.request\n","import warnings\n","warnings.filterwarnings('ignore', category=UserWarning, module='bs4')\n","\n","\n","import urllib.request\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n","data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7TUnygdeRBH","executionInfo":{"status":"ok","timestamp":1714459356630,"user_tz":-540,"elapsed":8913,"user":{"displayName":"JinSeok Song","userId":"14796603013551188771"}},"outputId":"734256df-52e1-4d39-97c5-53dffaac77bc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["print('전체 headline 데이터', len(data['headlines']))\n","print('유니크한 headline 데이터: ', (data['headlines'].nunique()))\n","print('전체 text 데이터', len(data['text']))\n","print('유니크한 text데이터', data['text'].nunique())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cIjIZsx3eiXd","executionInfo":{"status":"ok","timestamp":1714459357121,"user_tz":-540,"elapsed":505,"user":{"displayName":"JinSeok Song","userId":"14796603013551188771"}},"outputId":"d6785bd1-c56b-4b32-cf17-01e212161170"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["전체 headline 데이터 98401\n","유니크한 headline 데이터:  98280\n","전체 text 데이터 98401\n","유니크한 text데이터 98360\n"]}]},{"cell_type":"code","source":["data.drop_duplicates(subset=['text'], inplace=True)"],"metadata":{"id":"9DkwNKvVep2g","executionInfo":{"status":"ok","timestamp":1714459357121,"user_tz":-540,"elapsed":11,"user":{"displayName":"JinSeok Song","userId":"14796603013551188771"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["data.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8i13cybHerit","executionInfo":{"status":"ok","timestamp":1714459357121,"user_tz":-540,"elapsed":10,"user":{"displayName":"JinSeok Song","userId":"14796603013551188771"}},"outputId":"f9cc4f7f-08ea-41d9-febf-9aefb289956a"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["headlines    0\n","text         0\n","dtype: int64"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n","                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n","                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n","                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n","                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n","                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n","                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n","                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n","                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n","                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n","                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n","                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n","                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n","                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n","                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n","                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n","                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n","                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n","                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n","                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n","                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n","                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n","                           \"you're\": \"you are\", \"you've\": \"you have\"}\n","# 불용어 리스트 확인\n","print('불용어 개수 : ', len(stopwords.words('english')))\n","print(stopwords.words('english'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IIyafCtesrF","executionInfo":{"status":"ok","timestamp":1714459357121,"user_tz":-540,"elapsed":7,"user":{"displayName":"JinSeok Song","userId":"14796603013551188771"}},"outputId":"20452453-0e93-4054-f8ba-e145b8d3b412"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["불용어 개수 :  179\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","def preprocess_sentence(sentence, remove_stopwords=True):\n","  sentence = sentence.lower()\n","  sentence = BeautifulSoup(sentence, 'lxml').text # remove html tag\n","  sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 제거\n","  sentence = re.sub('\"', '', sentence)\n","  sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")])\n","  sentence = re.sub(r\"'s\\b\", \"\", sentence)\n","  sentence = re.sub(\"[^a-zA-Z0-9]\", \" \", sentence)\n","\n","#   # 단어 내에 같은 문자들이 3개 이상 연속해서 반복되면 삭제하고 최대 2개까지만 반복되도록 만들어줌 (ex. iiit -> iit)\n","#   b = []\n","#   for i in sentence.split():\n","#     a = list(i)\n","#     char_cnt = 0\n","#     for j in a:\n","#       if a.count(j) >= 3:\n","#         a.remove(j)\n","#     word_a = ''.join(a)\n","#     b.append(word_a)\n","#   sentence = ' '.join(b)\n","\n","\n","  # 불용어 제거\n","  if remove_stopwords:\n","    tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n","\n","  # 불용어 미제거\n","  else:\n","    tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n","  return tokens\n","# 샘플 문장을 확인\n","\n","temp_text = 'Everything I bought was great, infact I ordered iiiiit twice and the third ordered 123 was<br />for my mother and father. '\n","temp_summary = 'Great way to start (or finish) the day!!!'\n","\n","print(\"text: \", preprocess_sentence(temp_text))\n","print(\"summary:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXLLKdy1evD_","executionInfo":{"status":"ok","timestamp":1714459357123,"user_tz":-540,"elapsed":7,"user":{"displayName":"JinSeok Song","userId":"14796603013551188771"}},"outputId":"25932de4-75a3-4fca-ee45-ffdf1a006c66"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["text:  everything bought great infact ordered iiiiit twice third ordered 123 wasfor mother father\n","summary: great way to start the day\n"]}]},{"cell_type":"code","source":["# data['text'] 부분 전처리 진행\n","\n","clean_text = []\n","for s in data['text']:\n","  clean_text.append(preprocess_sentence(s))\n","\n","print('Text 전처리 후 결과 : ', clean_text[:5])"],"metadata":{"id":"-vTsQUggexiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# headline 부분 전처리 진행\n","clean_headlines = []\n","for s in data['headlines']:\n","  clean_headlines.append(preprocess_sentence(s, False))\n","\n","print('Headlines 전처리 후 결과 : ', clean_headlines[:5])"],"metadata":{"id":"VQ6SRHLVezF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['text'] = clean_text\n","data['headlines'] = clean_headlines\n","\n","data.replace('', np.nan, inplace=True)\n","\n","data.isnull().sum()"],"metadata":{"id":"61o9rWtShR5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","text_len = [len(x.split()) for x in data['text']]\n","headlines_len = [len(x.split()) for x in data['headlines']]\n","\n","print('텍스트 길이 최대: ', np.max(text_len))\n","print('텍스트 길이 최소: ', np.min(text_len))\n","print('텍스트 길이 평균: ', np.mean(text_len))\n","print('헤드라인 길이 최대: ', np.max(headlines_len))\n","print('헤드라인 길이 최소: ', np.min(headlines_len))\n","print('헤드라인 길이 평균: ', np.mean(headlines_len))\n","\n","plt.subplot(1,2,1)\n","plt.boxplot(text_len)\n","plt.title('Text')\n","plt.subplot(1,2,2)\n","plt.boxplot(headlines_len)\n","plt.title('Headlines')\n","plt.tight_layout()\n","plt.show()\n","\n","plt.title('Text')\n","plt.hist(text_len, bins = 40)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()\n","\n","plt.title('Headlines')\n","plt.hist(headlines_len, bins = 40)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"metadata":{"id":"oZuD5a33hT3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 텍스트, 헤드라인 최대 길이 설정\n","max_text_len = 60\n","max_headline_len = 16\n","\n","# 최대 길이를 기준으로 나눴을때 최대 길이보다 짧은 데이터의 비율을 출력해주는 함수 정의\n","def below_threshold_len(max_len, list):\n","  count = 0\n","  for s in list:\n","    if len(s.split()) <= max_len:\n","      count += 1\n","\n","  print(f'길이가 {max_len}보다 짧은 데이터의 비율 : {count/len(list)}')\n","\n","\n","# 비율 확인\n","below_threshold_len(max_text_len, data['text'])\n","below_threshold_len(max_headline_len, data['headlines'])"],"metadata":{"id":"FHwiGE36hWDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['decoder_input'] = data['headlines'].apply(lambda x : 'starttoken ' + x)\n","data['decoder_output'] = data['headlines'].apply(lambda x : x + ' endtoken')\n","\n","data.head()"],"metadata":{"id":"e5XuFU03haUb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_input = np.array(data['text'])\n","\n","decoder_input = np.array(data['decoder_input'])\n","decoder_output = np.array(data['decoder_output'])\n","\n","# 데이터 섞어주기\n","indices = np.arange(encoder_input.shape[0])\n","np.random.shuffle(indices)\n","\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_output = decoder_output[indices]\n","\n","# 샘플 추출\n","encoder_input[:5]"],"metadata":{"id":"szf4b-HlhdJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_size = int(encoder_input.shape[0] * 0.2)\n","\n","encoder_input_train = encoder_input[:-val_size]\n","decoder_input_train = decoder_input[:-val_size]\n","decoder_output_train = decoder_output[:-val_size]\n","\n","encoder_input_test = encoder_input[-val_size:]\n","decoder_input_test = decoder_input[-val_size:]\n","decoder_output_test = decoder_output[-val_size:]\n","\n","print('훈련 데이터의 개수 :', len(encoder_input_train))\n","print('훈련 레이블의 개수 :', len(decoder_input_train))\n","print('테스트 데이터의 개수 :', len(encoder_input_test))\n","print('테스트 레이블의 개수 :', len(decoder_input_test))"],"metadata":{"id":"NPjICoDJhgPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토크나이저 설정\n","src_tokenizer = Tokenizer()\n","src_tokenizer.fit_on_texts(encoder_input_train)\n","\n","# 단어 빈도수 체크\n","threshold = 4\n","total_cnt = len(src_tokenizer.word_index)\n","rare_cnt = 0\n","total_freq = 0\n","rare_freq = 0\n","\n","for key, value in src_tokenizer.word_counts.items():\n","  total_freq = total_freq + value\n","\n","  if value < threshold:\n","    rare_cnt += 1\n","    rare_freq = rare_freq + value\n","\n","\n","print('단어 집합(vocabulary)의 크기 :', total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"],"metadata":{"id":"w86YbSqshi6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src_vocab = 30847\n","src_tokenizer = Tokenizer(num_words = src_vocab)\n","src_tokenizer.fit_on_texts(encoder_input_train)\n","\n","# 정수 인코딩\n","# 텍스트 시퀀스를 정수 시퀀스로 변환\n","encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n","encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n","\n","# 샘플 출력하여 확인\n","print(encoder_input_train[:3])"],"metadata":{"id":"Wzl6Ei_Khktw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tar_tokenizer = Tokenizer()\n","tar_tokenizer.fit_on_texts(decoder_input_train)\n","\n","\n","threshold = 4\n","total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tar_tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :', total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"],"metadata":{"id":"Yk_1R-W9honz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tar_vocab = 13676\n","tar_tokenizer = Tokenizer(num_words=tar_vocab)\n","tar_tokenizer.fit_on_texts(decoder_input_train)\n","tar_tokenizer.fit_on_texts(decoder_output_train)\n","\n","# 텍스트 시퀀스를 정수 시퀀스로 변환\n","decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n","decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_output_train)\n","decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n","decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_output_test)\n","\n","# 잘 변환되었는지 확인\n","print('input')\n","print('input ',decoder_input_train[:5])\n","print('target')\n","print('decoder ',decoder_target_train[:5])"],"metadata":{"id":"2_gfVfB7hqeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n","drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n","\n","print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n","print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n","\n","encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n","decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n","decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n","\n","encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n","decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n","decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n","\n","print('훈련 데이터의 개수 :', len(encoder_input_train))\n","print('훈련 레이블의 개수 :', len(decoder_input_train))\n","print('테스트 데이터의 개수 :', len(encoder_input_test))\n","print('테스트 레이블의 개수 :', len(decoder_input_test))"],"metadata":{"id":"5nhHfQzZhtxG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_input_train = pad_sequences(encoder_input_train, maxlen=max_text_len, padding='post')\n","encoder_input_test = pad_sequences(encoder_input_test, maxlen=max_text_len, padding='post')\n","decoder_input_train = pad_sequences(decoder_input_train, maxlen=max_headline_len, padding='post')\n","decoder_target_train = pad_sequences(decoder_target_train, maxlen=max_headline_len, padding='post')\n","decoder_input_test = pad_sequences(decoder_input_test, maxlen=max_headline_len, padding='post')\n","decoder_target_test = pad_sequences(decoder_target_test, maxlen=max_headline_len, padding='post')"],"metadata":{"id":"0YLoeIwfhvP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EO5-agXNhwlN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WGF3wirIhxe-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# 인코더 설계 시작\n","embedding_dim = 256\n","hidden_size = 128\n","\n","# 인코더\n","encoder_inputs = Input(shape=(max_text_len,))\n","\n","# 인코더의 임베딩 층\n","enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n","\n","# 인코더의 LSTM 1\n","encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.5)\n","encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n","\n","# 인코더의 LSTM 2\n","encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.5)\n","encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n","\n","# 인코더의 LSTM 3\n","encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.5)\n","encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n","\n","\n","# 디코더 설계\n","decoder_inputs = Input(shape=(None, ))\n","\n","# 디코더의 임베딩 층\n","dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","\n","# 디코더의 LSTM\n","decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.5)\n","decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n","\n","# 디코더의 출력층\n","decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n","decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)\n","\n","\n","# 모델 정의\n","model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n","model.summary()"],"metadata":{"id":"Z81e5qoxhxhN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import AdditiveAttention\n","\n","# 어텐션 층(어텐션 함수)\n","attn_layer = AdditiveAttention(name='attention_layer')\n","\n","# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n","attn_out = attn_layer([decoder_outputs, encoder_outputs])\n","\n","# 어텐션의 결과와 디코더의 hidden_state들을 연결\n","decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n","\n","# 디코더의 출력층\n","decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n","decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n","\n","# 모델 정의\n","model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n","model.summary()"],"metadata":{"id":"GFqo-W7LhxjO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n","history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,\n","                    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n","                    batch_size=256, callbacks=[es], epochs=30)"],"metadata":{"id":"_LgGsciVh6id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"wuEMpOxlh-QN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어\n","tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수\n","tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어\n","\n","# 인코더\n","encoder_model = Model(inputs = encoder_inputs, outputs = [encoder_outputs, state_h, state_c])\n","\n","# 이전 시점의 상태들을 저장하는 텐서\n","decoder_state_input_h = Input(shape=(hidden_size, ))\n","decoder_state_input_c = Input(shape=(hidden_size, ))\n","\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# 문장의 다음 단어를 예측하기 위해서 초기 상태를 이전 시점의 상태로 사용\n","# 이는 뒤의 함수 decode_sequence()에 구현\n","# 훈련 과정에서와 달리 LSTM의 리턴하는 은틱 상태와 셀 상태인 state_h와 state_c를 버리지 않음\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","# 어텐션 함수\n","decoder_hidden_state_input = Input(shape=(max_text_len, hidden_size))\n","attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n","decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n","\n","# 디코더의 출력층\n","decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat)\n","\n","# 최종 디코더 모델\n","decoder_model = Model(\n","     [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n","     [decoder_outputs2] + [state_h2, state_c2])\n","\n","def decode_sequence(input_seq):\n","  # 입력으로부터 인코더의 상태를 얻음\n","  e_out, e_h, e_c = encoder_model.predict(input_seq)\n","\n","  # <SOS>에 해당하는 토큰 생성\n","  target_seq = np.zeros((1,1))\n","  target_seq[0,0] = tar_word_to_index['starttoken']\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","  while not stop_condition:\n","\n","    output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_token = tar_index_to_word[sampled_token_index]\n","\n","    if (sampled_token != 'endtoken'):\n","      decoded_sentence += ' '+sampled_token\n","\n","    # <EOS>에 도달하거나 최대 길이를 넘으면 중단.\n","    if (sampled_token == 'endtoken' or len(decoded_sentence.split()) >= (max_headline_len - 1)):\n","      stop_condition = True\n","\n","    # 길이가 1인 타겟 시퀀스를 업데이트\n","    target_seq = np.zeros((1,1))\n","    target_seq[0,0] = sampled_token_index\n","\n","    # 상태를 업데이트 합니다.\n","    e_h, e_c = h, c\n","\n","  return decoded_sentence"],"metadata":{"id":"vEAR1DwkiEey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq2text(input_seq):\n","    temp=''\n","    for i in input_seq:\n","        if (i!=0):\n","            temp = temp + src_index_to_word[i]+' '\n","    return temp\n","\n","# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq2summary(input_seq):\n","    temp=''\n","    for i in input_seq:\n","        if ((i!=0 and i!=tar_word_to_index['starttoken']) and i!=tar_word_to_index['endtoken']):\n","            temp = temp + tar_index_to_word[i] + ' '\n","    return temp"],"metadata":{"id":"8xVrh-KLiG_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(50, 60):\n","    print(\"원문 :\", seq2text(encoder_input_test[i]))\n","    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n","    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, max_text_len)))\n","    print(\"\\n\")"],"metadata":{"id":"5o_kmieAiQ8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_input_test[i].reshape(1, max_text_len)"],"metadata":{"id":"8vvkJWEwiSw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d9Paw9Xxinds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q0nnneszi4Ba"},"execution_count":null,"outputs":[]}]}